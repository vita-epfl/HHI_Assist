import torch
import torch.nn as nn
import math
import warnings
import random
import numpy as np
from collections import OrderedDict
from functools import partial
from itertools import repeat
# from lib.model.drop import DropPath


import copy
# from torch import nn
# from mlp import build_mlps
from einops.layers.torch import Rearrange

#ours:
from utils.hhi import HHI
from utils.misc_quat import quat_diff_rad

USE_DCT = False
USE_OFFSET = False

#new:
def get_dct_matrix(N):
    dct_m = np.eye(N)
    for k in np.arange(N):
        for i in np.arange(N):
            w = np.sqrt(2 / N)
            if k == 0:
                w = np.sqrt(1 / N)
            dct_m[k, i] = w * np.cos(np.pi * (i + 1 / 2) * k / N)
    idct_m = np.linalg.inv(dct_m)
    return dct_m, idct_m

dct_m, idct_m = get_dct_matrix(24)
dct_m = torch.tensor(dct_m).float().cuda().unsqueeze(0)
idct_m = torch.tensor(idct_m).float().cuda().unsqueeze(0)

def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., st_mode='vanilla'):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.mode = st_mode
        if self.mode == 'parallel':
            self.ts_attn = nn.Linear(dim*2, dim*2)
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        else:
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj_drop = nn.Dropout(proj_drop)

        self.attn_count_s = None
        self.attn_count_t = None

    def forward(self, x, seqlen=1):
        B, N, C = x.shape
        
        if self.mode == 'series':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_temporal(q, k, v, seqlen=seqlen)
        elif self.mode == 'parallel':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x_t = self.forward_temporal(q, k, v, seqlen=seqlen)
            x_s = self.forward_spatial(q, k, v)
            
            alpha = torch.cat([x_s, x_t], dim=-1)
            alpha = alpha.mean(dim=1, keepdim=True)
            alpha = self.ts_attn(alpha).reshape(B, 1, C, 2)
            alpha = alpha.softmax(dim=-1)
            x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]
        elif self.mode == 'coupling':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_coupling(q, k, v, seqlen=seqlen)
        elif self.mode == 'vanilla':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
        elif self.mode == 'temporal':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_temporal(q, k, v, seqlen=seqlen)
        elif self.mode == 'spatial':
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
        else:
            raise NotImplementedError(self.mode)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
    
    def reshape_T(self, x, seqlen=1, inverse=False):
        if not inverse:
            N, C = x.shape[-2:]
            x = x.reshape(-1, seqlen, self.num_heads, N, C).transpose(1,2)
            x = x.reshape(-1, self.num_heads, seqlen*N, C) #(B, H, TN, c)
        else:
            TN, C = x.shape[-2:]
            x = x.reshape(-1, self.num_heads, seqlen, TN // seqlen, C).transpose(1,2)
            x = x.reshape(-1, self.num_heads, TN // seqlen, C) #(BT, H, N, C)
        return x 

    def forward_coupling(self, q, k, v, seqlen=8):
        BT, _, N, C = q.shape
        q = self.reshape_T(q, seqlen)
        k = self.reshape_T(k, seqlen)
        v = self.reshape_T(v, seqlen)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v
        x = self.reshape_T(x, seqlen, inverse=True)
        x = x.transpose(1,2).reshape(BT, N, C*self.num_heads)
        return x

    def forward_spatial(self, q, k, v):
        B, _, N, C = q.shape
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v
        x = x.transpose(1,2).reshape(B, N, C*self.num_heads)
        return x
        
    def forward_temporal(self, q, k, v, seqlen=8):
        B, _, N, C = q.shape
        qt = q.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)
        kt = k.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)
        vt = v.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)

        attn = (qt @ kt.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ vt #(B, H, N, T, C)
        x = x.permute(0, 3, 2, 1, 4).reshape(B, N, C*self.num_heads)
        return x

    def count_attn(self, attn):
        attn = attn.detach().cpu().numpy()
        attn = attn.mean(axis=1)
        attn_t = attn[:, :, 1].mean(axis=1)
        attn_s = attn[:, :, 0].mean(axis=1)
        if self.attn_count_s is None:
            self.attn_count_s = attn_s
            self.attn_count_t = attn_t
        else:
            self.attn_count_s = np.concatenate([self.attn_count_s, attn_s], axis=0)
            self.attn_count_t = np.concatenate([self.attn_count_t, attn_t], axis=0)

class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., mlp_out_ratio=1., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, st_mode='stage_st', att_fuse=False):
        super().__init__()
        # assert 'stage' in st_mode
        self.st_mode = st_mode
        self.norm1_s = norm_layer(dim)
        self.norm1_t = norm_layer(dim)
        self.attn_s = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, st_mode="spatial")
        self.attn_t = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, st_mode="temporal")
        
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2_s = norm_layer(dim)
        self.norm2_t = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        mlp_out_dim = int(dim * mlp_out_ratio)
        self.mlp_s = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)
        self.mlp_t = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)
        self.att_fuse = att_fuse
        if self.att_fuse:
            self.ts_attn = nn.Linear(dim*2, dim*2)
    def forward(self, x, seqlen=1):
        if self.st_mode=='stage_st':
            x = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))
            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))
            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))
            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))
        elif self.st_mode=='stage_ts':
            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))
            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))
            x = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))
            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))
        elif self.st_mode=='stage_para':
            x_t = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))
            x_t = x_t + self.drop_path(self.mlp_t(self.norm2_t(x_t)))
            x_s = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))
            x_s = x_s + self.drop_path(self.mlp_s(self.norm2_s(x_s)))
            if self.att_fuse:
                #             x_s, x_t: [BF, J, dim]
                alpha = torch.cat([x_s, x_t], dim=-1)
                BF, J = alpha.shape[:2]
                # alpha = alpha.mean(dim=1, keepdim=True)
                alpha = self.ts_attn(alpha).reshape(BF, J, -1, 2)
                alpha = alpha.softmax(dim=-1)
                x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]
            else:
                x = (x_s + x_t)*0.5
        else:
            raise NotImplementedError(self.st_mode)
        return x
    
class DSTformer(nn.Module):   #base from MB
    def __init__(self, dim_in=3, dim_out=3, dim_feat=256, dim_rep=512,
                 depth=5, num_heads=8, mlp_ratio=4, 
                 num_joints=17, maxlen=243, 
                 qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, att_fuse=True):
        super().__init__()
        self.dim_out = dim_out
        self.dim_feat = dim_feat
        self.joints_embed = nn.Linear(dim_in, dim_feat)
        self.pos_drop = nn.Dropout(p=drop_rate)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks_st = nn.ModuleList([
            Block(
                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, 
                st_mode="stage_st")
            for i in range(depth)])
        self.blocks_ts = nn.ModuleList([
            Block(
                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, 
                st_mode="stage_ts")
            for i in range(depth)])
        self.norm = norm_layer(dim_feat)
        if dim_rep:
            self.pre_logits = nn.Sequential(OrderedDict([
                ('fc', nn.Linear(dim_feat, dim_rep)),
                ('act', nn.Tanh())
            ]))
        else:
            self.pre_logits = nn.Identity()
        self.head = nn.Linear(dim_rep, dim_out) if dim_out > 0 else nn.Identity()            
        self.temp_embed = nn.Parameter(torch.zeros(1, maxlen, 1, dim_feat))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_joints, dim_feat))
        trunc_normal_(self.temp_embed, std=.02)
        trunc_normal_(self.pos_embed, std=.02)
        self.apply(self._init_weights)
        self.att_fuse = att_fuse
        if self.att_fuse:
            self.ts_attn = nn.ModuleList([nn.Linear(dim_feat*2, 2) for i in range(depth)])
            for i in range(depth):
                self.ts_attn[i].weight.data.fill_(0)
                self.ts_attn[i].bias.data.fill_(0.5)
                

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_classifier(self):
        return self.head

    def reset_classifier(self, dim_out, global_pool=''):
        self.dim_out = dim_out
        self.head = nn.Linear(self.dim_feat, dim_out) if dim_out > 0 else nn.Identity()

    def forward(self, x, return_rep=False):   
        

        offset = x[:, -1:] if USE_OFFSET else 0 #new

        B0, F0, J0, C0 = x.shape #new
        B, F, J, C = x.shape
        
        if USE_DCT:
            x = x.reshape(B,F,J*C)
            x = torch.matmul(dct_m[:, :, :24], x.cuda())
            x = x.reshape(B,F,J,C)
            
        x = x.reshape(-1, J, C)
        BF = x.shape[0]
        x = self.joints_embed(x)
        x = x + self.pos_embed
        _, J, C = x.shape
        x = x.reshape(-1, F, J, C) + self.temp_embed[:,:F,:,:]
        x = x.reshape(BF, J, C)
        x = self.pos_drop(x)
        alphas = []
        for idx, (blk_st, blk_ts) in enumerate(zip(self.blocks_st, self.blocks_ts)):
            x_st = blk_st(x, F)
            x_ts = blk_ts(x, F)
            if self.att_fuse:
                att = self.ts_attn[idx]
                alpha = torch.cat([x_st, x_ts], dim=-1)
                BF, J = alpha.shape[:2]
                alpha = att(alpha)
                alpha = alpha.softmax(dim=-1)
                x = x_st * alpha[:,:,0:1] + x_ts * alpha[:,:,1:2]
            else:
                x = (x_st + x_ts)*0.5
        x = self.norm(x)
        x = x.reshape(B, F, J, -1)
        x = self.pre_logits(x)         # [B, F, J, dim_feat]
        if return_rep:
            return x
        x = self.head(x)
        
        #new
        if USE_DCT:
            x = x.reshape(B0, F0, J0*C0)
            x = torch.matmul(idct_m[:, :, :24], x.cuda())
            x = x.reshape(B0, F0, J0, C0)
        
        if USE_OFFSET:
            x = x + offset
        #end new
        
        #add a brekapoint if there is nan in output:
        if torch.isnan(x).any():
            breakpoint()
        
        return x

    def get_representation(self, x):
        return self.forward(x, return_rep=True)
    

class baseline(nn.Module):
    def __init__(self, device, maxlen,  args = None) : #options: "mb", "mlp", "lstm"
        super().__init__()
        
        self.angles = args.angles
        baseline_typ = args.baseline
        self.num_joints = args.joints
        
        print("Baseline type: ", baseline_typ)
        
        maxlen = maxlen//2
        
        if args.angles=="RotMatrix":
            dim_in = 9
        elif args.angles=="Quat":
            dim_in = 4
        elif args.angles=="Euler":
            dim_in = 3
        else:
            dim_in = 3
                    
        if baseline_typ == "mb":
            self.model = DSTformer(dim_in= dim_in, dim_out=dim_in, dim_feat=256, dim_rep=512,
                 num_joints=self.num_joints, maxlen=maxlen)
        elif baseline_typ == "mlp":
            self.model = MLP_baseline_arch(maxlen, num_joints=self.num_joints)
        elif baseline_typ == "lstm":
            self.model = LSTM_baseline_arch(maxlen, num_joints=self.num_joints)
        elif baseline_typ == "simlpe":
            print("Using siMLPe")
            self.model = siMLPe(dim_in=dim_in, joints=self.num_joints)
        elif baseline_typ == "zv":
            self.model = ZeroVel(device, pred_len=args.output_n)
        elif baseline_typ == "cv":
            self.model = ConstVel(device, pred_len=args.output_n)
        else:
            assert False, "Invalid baseline type"
            # self.model = torch.nn.Identity()
            # self.model = MLP_baseline_arch(maxlen, num_joints=self.num_joints)
        
        self.device = device
        self.model = self.model.to(device)

        if args.angles == "Quat":
            self.loss = partial(self.custom_loss_quat)
        else:
            self.loss = nn.MSELoss() #default loss function

    def custom_loss_quat(self, y_pred, y_gt):
                
        B, T, J, C = y_pred.shape
        assert C == 4, "Each quaternion must have 4 components."
        
        # y_pred = y_pred.view(B, T, J*C)
        # y_gt = y_gt.view(B, T, J*C)
        
        # y_pred = y_pred.permute(0, 2, 1)
        # y_gt = y_gt.permute(0, 2, 1)
        
        # Ensure that the quaternion dimensions are correct after permutation
        # assert y_pred.shape[1] % 4 == 0, "Quaternion dimensions misaligned."
        
        loss = quat_diff_rad(y_pred, y_gt)
        
        return loss

    def forward(self, batch, is_train=1,  angles=None) :
        batch_pose = torch.Tensor(batch['pose']).float()
        batch_pose = batch_pose.to(self.device)
                
        B, T, JC = batch_pose.shape
        J = self.num_joints
        
        x = batch_pose[:,:T//2] #(B, 48, 60) where 48 is 24 obs and 24 pred
        y_gt = batch_pose[:, T//2:]
        
        # print(y_gt.max(), y_gt.min())

        x = x.view(B, T//2, J, JC//J)
        y_gt = y_gt.view(B, T//2, J, JC//J) #i.e.: (B, 24, 21, 3)
        
        y_pred = self.model(x)
        
        if self.angles == "Quat": #TBV
            #deviding to make the norm 1:
            y_pred = y_pred / y_pred.norm(dim=-1, keepdim=True)
        
        loss = self.loss(y_pred, y_gt)
        
        return loss
    
    @torch.no_grad()    
    def evaluate(self, batch, nsample=None, args=None) :
        
        self.model.eval()
        
        batch = batch['pose'].float()
        batch = batch.to(self.device)
        B, T, JC = batch.shape
        J = self.num_joints
        x = batch[:,:T//2]
        x = x.view(B ,T//2 ,J ,JC//J)

        preds = self.model(x) 
        
        if self.angles == "Quat": #TBV
            #deviding to make the norm 1: 
            preds = preds / preds.norm(dim=-1, keepdim=True)
        
        return preds
    
    
    
#mlp baseline:

class MLP_baseline_arch(nn.Module):
    def __init__(self, seq_len, num_joints) :
        super().__init__()
        
        input_dim = seq_len*num_joints*3
        
        self.model = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, input_dim)
        )
        
    def forward(self, x) :
        B, T, J, C = x.shape
        x = x.view(B, T*J*C)
        x = self.model(x)
        x = x.view(B, T, J, C)
        return x
 
class LSTM_baseline_arch(nn.Module):
    def __init__(self, seq_len, num_joints):
        super().__init__()
        
        self.input_size = num_joints*3
        self.hidden_size = 256
        self.num_layers = 2
        
        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)
        self.fc = nn.Linear(self.hidden_size, self.input_size)
        
        
    def forward(self, x):
        
        device = x.device
        
        B, T, J, C = x.shape
        x = x.view(B, T, J*C)
        
        h_0 = torch.zeros(self.num_layers, B, self.hidden_size).to(device)
        c_0 = torch.zeros(self.num_layers, B, self.hidden_size).to(device)
        
        x_last = x[:,-1,:]
                
        output, _ = self.lstm(x, (h_0, c_0))
        
        output = self.fc(output) #[:, -1, :]
        
        output = output.view(B, T, J*C)
        
        output = output + x_last.unsqueeze(1)
        
        output = output.view(B, T, J, C)
        
        return output
        

class ConstVel(nn.Module):
    def __init__(self, device, pred_len):
        super().__init__()
        self.device = device
        self.pred_len = pred_len
        
    def forward(self, x):
        B, T, J, C = x.shape
        
        samples = x.clone()
        pred = torch.empty((B, self.pred_len, J, C)).to(self.device)
        
        increment_to_put = samples[:,-1] - samples[:,-2]
        pred[:,0] = samples[:,-1] + increment_to_put
        
        print(increment_to_put.max().item())
        
        for i in range(1, self.pred_len):
            pred[:, i] = pred[:, i-1] + increment_to_put
        return pred

class ZeroVel(nn.Module):
    def __init__(self, device, pred_len):
        super().__init__()
        self.device = device
        self.pred_len = pred_len
        
        
    def forward(self, x):
        B, T, J, C = x.shape
        samples = x.clone()
        #repeate the last sample for all the future samples:
        pred = samples[:,-1].unsqueeze(1).expand(-1, self.pred_len, -1, -1).to(self.device)
        
        return pred


#_______________________ 

class LN(nn.Module):
    def __init__(self, dim, epsilon=1e-5):
        super().__init__()
        self.epsilon = epsilon

        self.alpha = nn.Parameter(torch.ones([1, dim, 1]), requires_grad=True)
        self.beta = nn.Parameter(torch.zeros([1, dim, 1]), requires_grad=True)

    def forward(self, x):
        mean = x.mean(axis=1, keepdim=True)
        var = ((x - mean) ** 2).mean(dim=1, keepdim=True)
        std = (var + self.epsilon).sqrt()
        y = (x - mean) / std
        y = y * self.alpha + self.beta
        return y

class LN_v2(nn.Module):
    def __init__(self, dim, epsilon=1e-5):
        super().__init__()
        self.epsilon = epsilon

        self.alpha = nn.Parameter(torch.ones([1, 1, dim]), requires_grad=True)
        self.beta = nn.Parameter(torch.zeros([1, 1, dim]), requires_grad=True)

    def forward(self, x):
        mean = x.mean(axis=-1, keepdim=True)
        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)
        std = (var + self.epsilon).sqrt()
        y = (x - mean) / std
        y = y * self.alpha + self.beta
        return y

class Spatial_FC(nn.Module):
    def __init__(self, dim):
        super(Spatial_FC, self).__init__()
        self.fc = nn.Linear(dim, dim)
        self.arr0 = Rearrange('b n d -> b d n')
        self.arr1 = Rearrange('b d n -> b n d')

    def forward(self, x):
        x = self.arr0(x)
        x = self.fc(x)
        x = self.arr1(x)
        return x

class Temporal_FC(nn.Module):
    def __init__(self, dim):
        super(Temporal_FC, self).__init__()
        self.fc = nn.Linear(dim, dim)

    def forward(self, x):
        x = self.fc(x)
        return x

class MLPblock(nn.Module):

    def __init__(self, dim, seq, use_norm=True, use_spatial_fc=False, layernorm_axis='spatial'):
        super().__init__()

        if not use_spatial_fc:
            self.fc0 = Temporal_FC(seq)
        else:
            self.fc0 = Spatial_FC(dim)

        if use_norm:
            if layernorm_axis == 'spatial':
                self.norm0 = LN(dim)
            elif layernorm_axis == 'temporal':
                self.norm0 = LN_v2(seq)
            elif layernorm_axis == 'all':
                self.norm0 = nn.LayerNorm([dim, seq])
            else:
                raise NotImplementedError
        else:
            self.norm0 = nn.Identity()

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.fc0.fc.weight, gain=1e-8)

        nn.init.constant_(self.fc0.fc.bias, 0)

    def forward(self, x):

        x_ = self.fc0(x)
        x_ = self.norm0(x_)
        x = x + x_

        return x

class TransMLP(nn.Module):
    def __init__(self, dim, seq, use_norm, use_spatial_fc, num_layers, layernorm_axis):
        super().__init__()
        self.mlps = nn.Sequential(*[
            MLPblock(dim, seq, use_norm, use_spatial_fc, layernorm_axis)
            for i in range(num_layers)])

    def forward(self, x):
        x = self.mlps(x)
        return x

def build_mlps(args):
    
    if 'seq_len' in args:
        seq_len = args["seq_len"]
    else:
        seq_len = None
        
    return TransMLP(
        dim=args["hidden_dim"],
        seq=seq_len,
        use_norm=args["with_normalization"],
        use_spatial_fc=args["spatial_fc_only"],
        num_layers=args["num_layers"],
        layernorm_axis=args["norm_axis"],
    ) 
    
    # if 'seq_len' in args:
    #     seq_len = args.seq_len
    # else:
    #     seq_len = None   
    # return TransMLP(
    #     dim=args.hidden_dim,
    #     seq=seq_len,
    #     use_norm=args.with_normalization,
    #     use_spatial_fc=args.spatial_fc_only,
    #     num_layers=args.num_layers,
    #     layernorm_axis=args.norm_axis,
    # )
    
#
class siMLPe(nn.Module):   #Main model class for siMLPE (from: https://github.com/dulucas/siMLPe)
    def __init__(self, dim_in, joints, config=None):
        # self.config = copy.deepcopy(config)
        
        h36m_input_length_dct_config = 24 #50
        dim_config = dim_in*joints #66
        temporal_fc_config = False
        
        super(siMLPe, self).__init__()
        
        # seq = self.config.motion_mlp.seq_len
        seq = h36m_input_length_dct_config 
        
        self.arr0 = Rearrange('b n d -> b d n')
        self.arr1 = Rearrange('b d n -> b n d')

        non_easy_dict = {'hidden_dim': dim_config, 'seq_len': seq, 'with_normalization': True, 
                         'spatial_fc_only': False, 'num_layers':48, 'norm_axis': 'spatial'}
        self.motion_mlp = build_mlps(non_easy_dict) #self.config.motion_mlp)

        self.temporal_fc_in = temporal_fc_config
        self.temporal_fc_out = temporal_fc_config
        if self.temporal_fc_in:
            self.motion_fc_in = nn.Linear(h36m_input_length_dct_config, h36m_input_length_dct_config)
        else:
            self.motion_fc_in = nn.Linear(dim_config, dim_config)
        if self.temporal_fc_out:
            self.motion_fc_out = nn.Linear(h36m_input_length_dct_config, h36m_input_length_dct_config)
        else:
            self.motion_fc_out = nn.Linear(dim_config, dim_config)

        self.reset_parameters()
        

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.motion_fc_out.weight, gain=1e-8)
        nn.init.constant_(self.motion_fc_out.bias, 0)

    def forward(self, motion_input):
        #new:
        B, S, J, C = motion_input.shape
        offset = motion_input[:, -1:] if USE_OFFSET else 0
        motion_input = motion_input.view(B, S, J*C) 
        
        h36m_motion_input_ = motion_input.clone()
        if USE_DCT:
            h36m_motion_input_ = torch.matmul(dct_m[:, :, :S], h36m_motion_input_.cuda())
        #end new

        if self.temporal_fc_in:
            motion_feats = self.arr0(h36m_motion_input_)
            motion_feats = self.motion_fc_in(motion_feats)
        else:
            motion_feats = self.motion_fc_in(h36m_motion_input_)
            motion_feats = self.arr0(motion_feats)

        motion_feats = self.motion_mlp(motion_feats)

        if self.temporal_fc_out:
            motion_feats = self.motion_fc_out(motion_feats)
            motion_feats = self.arr1(motion_feats)
        else:
            motion_feats = self.arr1(motion_feats)
            motion_feats = self.motion_fc_out(motion_feats)
        
        #new:     
        if USE_DCT:
            motion_feats = torch.matmul(idct_m[:, :S, :], motion_feats)
        
        motion_feats = motion_feats.view(B, S, J, C) 
        motion_feats = motion_feats + offset
        
        #end new
        
        if torch.isnan(motion_feats).any():
            breakpoint()
            print(motion_input)

        return motion_feats

#___________________________


if __name__ == '__main__':
    
    model = siMLPe(3,20)
    print(model)
    
    x = torch.randn(2, 24 , 20, 3)
    y = model(x)
    print(y.shape)